# ğŸš€ RAG Application (Gemini)

## âœ¨ Overview

**RAG Application (Gemini)** is an interactive Streamlit-based web app that allows users to upload PDFs and ask questions directly from the document.

It uses **Retrieval-Augmented Generation (RAG)** powered by **Google Gemini**, **LangChain**, and **FAISS** to generate accurate, context-aware answers strictly from the uploaded document.

ğŸ’¡ If the answer is not present in the PDF, the model clearly responds:

> "Answer not available in the provided PDF."

---

## ğŸ¯ Features

- ğŸ“‚ Upload any PDF document
- âœ‚ï¸ Automatic text chunking
- ğŸ§  HuggingFace Embeddings (`all-MiniLM-L6-v2`)
- ğŸ” FAISS Vector Database
- ğŸ¤– Gemini 2.5 Flash LLM
- ğŸ“š Context-based answering only
- âŒ Proper fallback response when answer not found

---

## ğŸ§° Technologies Used

- **Python**
- **Streamlit**
- **LangChain**
- **FAISS**
- **HuggingFace Sentence Transformers**
- **Google Gemini API**
- **PyPDF2**

---

# âš™ï¸ Setup Instructions

Follow the steps below to run the project locally.

---

## 1 Clone the Repository

```bash
git clone https://github.com/YOUR_USERNAME/YOUR_REPOSITORY_NAME.git
cd YOUR_REPOSITORY_NAME
```
## 2 Install Requirements

```bash
pip install -r requirements.txt
```

## 3 Add Gemini API Key

```bash
GOOGLE_API_KEY=your_gemini_api_key_here
```

## 4 Run the Application

```bash
streamlit run app.py
```


## ğŸ§  How It Works (RAG Flow)

Hereâ€™s what happens internally:

1ï¸âƒ£ User uploads a PDF
2ï¸âƒ£ Text is extracted using PyPDF2
3ï¸âƒ£ Text is split into chunks
4ï¸âƒ£ Embeddings are generated using HuggingFace
5ï¸âƒ£ FAISS stores vector embeddings
6ï¸âƒ£ User asks a question
7ï¸âƒ£ Relevant chunks are retrieved
8ï¸âƒ£ Gemini generates an answer using ONLY the retrieved context





Author
Lucky
